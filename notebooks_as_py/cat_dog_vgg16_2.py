# -*- coding: utf-8 -*-
"""cat_dog_vgg16_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16iYWzjh41maYOdNGDFo1FjCXgjhvMFRC

<h1 align=center><font size = 6>Classify Cat and Dog Images with VGG-16</font></h1>

<img src="https://d.newsweek.com/en/full/1809693/cat-dog.jpg?w=1600&h=900&q=88&f=772f894f001bafc6c5094cc33d71bc19" width=1000 height=450 alt="https://d.newsweek.com/"/>

<small>Picture Source: <a href="https://www.newsweek.com/cat-becomes-dog-copying-behavior-adorable-video-1595916">Newsweek</a></small>

<br>

<h2>Objective</h2>

<p>The objective of this project is to develop a deep learning model using the VGG16 architecture to classify images of cats and dogs with high accuracy. The model will be trained on a dataset of labeled images and evaluated on a separate test set. The performance of the model will be measured using metrics such as accuracy, precision, recall, and F1 score. The ultimate goal is to create a model that can accurately distinguish between images of cats and dogs in real-world scenarios.</p>

<br>

<h2>Context</h2>

<p>The dataset consists of 1,425 digital images of domestic animals, specifically 713 images of dogs and 712 images of cats. These images were collected for the purpose of training a deep learning model to classify images of cats and dogs with high accuracy. All images were obtained from various online sources and manually labeled by experts to ensure accuracy of the dataset. The images vary in size and resolution, and were preprocessed to remove any artifacts or unwanted information. </p>

<br><a href=''></a>

<h2>Table of Contents</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">
  <ul>
    <li><a href="https://#appropriate_environment">Create the Appropriate Environment</a></li>
    <li><a href="https://#libraries">Import Libraries and Packages </a></li>
    <li><a href="https://#data_preparation">Dataset Preparation</a></li>
    <li><a href="https://#compile_fit">Compile VGG-16 Model</a></li>
    <li><a href="https://#train_model">Train the VGG16 Model</a></li>
    <li><a href="https://#evaluate_model">Evaluate the VGG16 Model</a></li>
    <li><a href="https://#upload_pred">Upload and Predict Your Picture!</a></li>
  </ul>

  <br>

  <p>Estimated Time Needed: <strong>60 min</strong></p>

</div>

<br>

<h2 align=center id="appropriate_environment">Create the Appropriate Environment</h2>

<p>After downloading the dataset, we can create the appropriate environment for the data and we can unzip the file.</p>
"""

!mkdir cat_dog_classification

# Commented out IPython magic to ensure Python compatibility.
# %cd cat_dog_classification

!unzip -q /content/drive-download-20230504T091834Z-001.zip

import shutil
import os
import numpy as np

def train_test_split(val_ratio=0.1, test_ratio=0.2):

    print("Splitting started!")

    root_dir = '/content/'
    classes_dir = ['cats', 'dogs']
    processed_dir = '/content/cat_dog_classification'

    for cls in classes_dir:

        print("\nClass Name " + cls)
        src = processed_dir +"//" + cls
        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames), [int(len(allFileNames) * (1 - (val_ratio + test_ratio))), int(len(allFileNames) * (1 - val_ratio)),])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)
        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)
        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("\nComplete!")

train_test_split()

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

!ls

"""<p>You may want to remove zip file which contains our dataset.</p>"""

# !rm -rf /content/drive-download-20230504T091834Z-001.zip

"""<br>

<h2 align=center id="libraries">Import Libraries and Packages</h2>

<p>Before we proceed, let's import the libraries and packages that we will need to complete the rest of this lab.</p>
"""

import matplotlib.pyplot as plt
import pandas as pd
import datetime

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import TensorBoard, CSVLogger, ModelCheckpoint

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing import image
from keras.utils import load_img, img_to_array

from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc

import warnings
warnings.filterwarnings("ignore")

tf.__version__

"""<br>

<h2 align=center id="data_preparation">Dataset Preparation</h2>

<br>
<h3>Define Parameters</h3>
"""

#@markdown ---
#@markdown ### Enter number of classes:
num_classes = 1 #@param {type:"integer"}

image_resize = (224, 224)

#@markdown

#@markdown ### Enter number of epochs:
num_epochs = 8 #@param {type:"integer"}

#@markdown

#@markdown ### Enter batch sizes:
batch_size_training = 8 #@param {type:"integer"}
batch_size_validation = 4 #@param {type:"integer"}
batch_size_test = 2 #@param {type:"integer"}

#@markdown 

#@markdown ### Enter director paths:
train_path = '/content/train' #@param {type:"string"}
valid_path = '/content/val' #@param {type:"string"}
test_path = '/content/test' #@param {type:"string"}
#@markdown ---

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

"""<p>We are going to separate our dataset as <i>1068</i> files for training and <i>355</i> files for validation.</p>

<br>
<h3>Prepare Train Class</h3>
"""

train_generator = datagen.flow_from_directory(
    train_path,
    target_size=image_resize,
    batch_size=batch_size_training,
    class_mode='binary',
)

print('Class indices:', train_generator.class_indices)
print('Number of train samples:', train_generator.samples)
print('First and last 5 filenames:', train_generator.filenames[0:5], train_generator.filenames[-5:])
# print('Train classes: ', train_generator.classes)

labels = list(train_generator.class_indices.keys())
freqs = np.mean(train_generator.labels, axis=0)

fig, ax = plt.subplots(figsize=(20, 5))
ax.bar(labels, freqs, color=['green', 'red'])
ax.set_xlabel('Class Labels')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Class Labels in Training Set')
plt.xticks(rotation=90)
plt.show()

"""<p>A balanced dataset is a dataset in which all classes or categories have approximately the same number of samples. This means that no class is overrepresented or underrepresented in the dataset. In other words, a balanced dataset has a uniform distribution of samples across all classes.</p>

<p>Having a balanced dataset is important for training machine learning models because it ensures that the model is exposed to an equal number of examples from all classes.</p>
"""

labels = train_generator.classes
unique_labels, label_counts = np.unique(labels, return_counts=True)

print("Number of unique labels:", len(unique_labels))
for label, count in zip(unique_labels, label_counts):
    print("Label:", label, "- Count:", count)

"""<br>
<h4>Dive Into Batches</h4>

<p>The batch size refers to the number of samples that will be processed by the model in each training iteration. For example, if the batch size is set to 32, then the model will process 32 samples at a time during each iteration. Larger batch sizes can lead to faster training times, but can also require more memory and may be less accurate. Smaller batch sizes can be slower, but may result in better generalization and less overfitting.</p>
"""

first_batch = train_generator.next()

first_batch[1]

fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))

ind = 0
for ax1 in axs:
    for ax2 in ax1: 
        image_data = first_batch[0][ind]
        ax2.imshow(image_data)
        ind += 1

fig.suptitle('First Batch of Train Images') 
plt.show()

"""<br>
<h3>Prepare Validation Class</h3>
"""

validation_generator = datagen.flow_from_directory(
    valid_path,
    target_size=(224, 224),
    batch_size=batch_size_validation,
    class_mode='binary',
)

print('Class indices:', validation_generator.class_indices)
print('Number of validation samples:', validation_generator.samples)
print('First and last 5 filenames:', validation_generator.filenames[0:5], validation_generator.filenames[-5:])
# print('Validation classes: ', validation_generator.classes)

"""<br>

<p>The "Distribution of Class Labels in Training Set" plot shows the distribution of samples across the different classes or categories in the <i>training set</i>. The <i>x-axis</i> represents the different classes, and the y-axis represents the frequency or number of <i>samples</i> in each <i>class</i>. Each <i>bar</i> in the plot represents a class, and its height corresponds to the number of samples in that class.</p>

<br>

<p>By <i>examining</i> this <i>plot</i>, you can get a sense of how <i>balanced</i> or <i>imbalanced</i> your <i>dataset</i> is. If the height of the bars is roughly the same for all classes, then your <i>dataset</i> is considered to be balanced. If some bars are much taller than others, then your dataset is considered to be <i>imbalanced</i>, with some <i>classes</i> having more samples than others.</p>

<br>

<p>This plot is helpful for understanding the underlying distribution of your dataset and identifying any potential <i>imbalances</i> that may affect your model's performance. <b>If your <i>dataset</i> is <i>imbalanced</i>, you may need to consider using techniques such as <i>oversampling</i> or <i>undersampling</i> to create a more balanced dataset</b>.</p>
"""

labels = list(validation_generator.class_indices.keys())
freqs = np.mean(validation_generator.labels, axis=0)

fig, ax = plt.subplots(figsize=(20, 5))
ax.bar(labels, freqs, color=['blue', 'orange'])
ax.set_xlabel('Class Labels')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Class Labels in Training Set')
plt.xticks(rotation=90)
plt.show()

labels = validation_generator.classes
unique_labels, label_counts = np.unique(labels, return_counts=True)

print("Number of unique labels:", len(unique_labels))
for label, count in zip(unique_labels, label_counts):
    print("Label:", label, "- Count:", count)

"""<p>Having a <i>balanced dataset</i> is important for training <i>machine learning models</i> because it ensures that the model is exposed to an equal number of examples from all classes. This is particularly important in <i>classification problems</i>, where the goal is to <i>predict</i> the correct <i>class label</i> for a given input sample. If a <i>dataset</i> is imbalanced, where some classes have much fewer examples than others, the <i>model</i> may learn to be <i>biased</i> towards the <i>majority class</i> and perform poorly on the <i>minority classes</i>.</p>

<p><i>Balancing</i> a <i>dataset</i> can be achieved through various techniques, such as <b><i>oversampling the minority classes</i>, <i>undersampling the majority classes</i>, or using a <i>combination</i> of both</b>. These techniques aim to <i>adjust</i> the number of samples in each class to create a more <i>balanced dataset</i>.</p>

<br>
<h4>Dive Into Batches</h4>

<p>The <i>batch</i> size determines the number of <i>samples</i> processed in each <i>iteration</i>.</p>
"""

first_batch = validation_generator.next()

first_batch[1]

fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(20, 10))

ind = 0
for ax1 in axs: 
    image_data = first_batch[0][ind]
    ax1.imshow(image_data)
    ind += 1

fig.suptitle('First Batch of Validation Images') 
plt.show()

"""<br>

<h2 align=center id="compile_fit">Compile VGG-16 Model</h2>

<p><i>VGG-16</i> is a <i>convolution neural net (CNN)</i> architecture which was used to win <i>ILSVR (Imagenet)</i> competition in 2014. It is considered to be one of the excellent <i>vision model architecture</i> till date. In this section, we will start building our model. We will use the <i>Sequential model class</i> from <i>Keras</i>.</p></p>

<br>
<h3>Build VGG-16 Model</h3>
"""

model_vgg16 = Sequential()

"""<p>Then, we will define our output layer as a <b>Dense</b> layer.</p>"""

model_vgg16.add(VGG16(include_top=False, pooling="avg", weights="imagenet"))
model_vgg16.add(Dense(64, activation='softmax', name = 'dense'))
model_vgg16.add(Dense(num_classes, activation="sigmoid", name = 'output'))

model_vgg16.layers[0].trainable = False

"""<p>You can access the model's layers using the <i>layers</i> attribute of our model object.</p>"""

model_vgg16.layers

"""<p>And now using the <i>summary</i> attribute of the model.</p>"""

model_vgg16.summary()

plot_model(model_vgg16, to_file='model_vgg16.png', show_shapes=True, show_layer_names=True)

"""<br>
<h3>Compile the Model</h3>

<p>Next, we compile our model using the <b>adam</b> optimizer.</p>
"""

model_vgg16.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""<br>

<h2 align=center id="train_model">Train the VGG16 Model</h2>

<br>
<h3>Define Callback</h3>

<a name="4-1-1"></a>
#### [TensorBoard](https://www.tensorflow.org/tensorboard?hl=tr)

TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more. 

<br>

<a name="4-1-2"></a>
#### [CSVLogger](https://keras.io/api/callbacks/csv_logger/)

Callback that streams epoch results to a CSV file.

<br>

<a name="4-1-3"></a>
#### [ModelCheckpoint](https://keras.io/api/callbacks/model_checkpoint/)

ModelCheckpoint callback is used in conjunction with training using <code>model.fit()</code> to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.
"""

get_ipython().system('rm -rf logs')

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = TensorBoard(logdir)

csv_file = 'training.csv'

callbacks_list = [tensorboard_callback, CSVLogger(csv_file), ModelCheckpoint('/content/' + 'best_model.h5', verbose=1)]

"""<br>
<h3>Fit the Model</h3>
"""

import time
start = time.time()

history = model_vgg16.fit(train_generator, 
                    epochs=num_epochs,
                    validation_data=validation_generator,
                    callbacks=[callbacks_list])

end = time.time()

elapsed_time = end - start

print(f"Elapsed Time:{elapsed_time}s")

"""<br>
<h3>Save the Model</h3>
"""

model_vgg16.save('/content/')

"""<br>

<h2 align=center id="evaluate_model">Evaluate the VGG16 Model</h2>

<p>Evaluating a CNN model is a crucial step in the development of a deep learning project. It helps in understanding how well the model performs on unseen data and provides insights into whether the model is overfitting or underfitting. Evaluation of the CNN model is important for the following reasons:</p>

<br>

<ol>
  <li><b>Performance measurement:</b> Evaluating a model helps in measuring its performance on the test data. This gives an idea of how well the model has learned to generalize to new data.</li>

  <li><b>Model selection:</b> Evaluation of different models can help in selecting the best model among them. It is essential to choose the model with the best performance metrics to deploy it in a production environment.</li>

  <li><b>Hyperparameter tuning:</b> Evaluating a model can also help in fine-tuning the hyperparameters of the model. By comparing the performance of models with different hyperparameters, we can identify the optimal hyperparameters that produce the best performance.</li>

  <li><b>Debugging:</b> Evaluation helps in identifying and diagnosing issues in the model. It can help in determining whether the model is overfitting or underfitting, which can be addressed by adjusting the architecture or hyperparameters.</li>
</ol>

<br>
<h3>Evaluate the Model with TensorBoard</h3>
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

get_ipython().run_line_magic('tensorboard', '--logdir logs')

# from tensorflow.keras.models import load_model
# model = load_model('/content/best_model.h5')

"""<br>
<h3>Calculate Model Loss and Accuracy</h3>

<p>Evaluate the model on the test set.</p>
"""

test_generator = datagen.flow_from_directory(
  test_path,
  target_size=image_resize,
  batch_size=batch_size_test,
  class_mode='binary',
  shuffle=False,
)

test_loss, test_acc = model_vgg16.evaluate(test_generator)
print('loss ', test_loss, 'test accuracy ', test_acc)

"""<br>
<h3>Predict Test Images and Show Them on DataFrame</h3>
"""

filenames=test_generator.filenames
pred=model_vgg16.predict_generator(test_generator, steps=len(test_generator), verbose=1).round(3)

filenames_df = pd.DataFrame(filenames, columns=['File Path'])
pred_df = pd.DataFrame(np.round(pred), columns=['Predicted Indice'])
model_predictions = pd.concat([filenames_df, pred_df], axis=1)
model_predictions

file_name='model_predictions.csv'
model_predictions.to_csv(file_name, sep=',', encoding='utf-8')

"""<br>
<h3>Confusion Matrix</h4>

<p>Get the predicted labels for the test set.</p>
"""

y_pred = model_vgg16.predict(test_generator)

y_pred = np.round(y_pred)

"""<p>Get the true labels for the test set.</p>"""

y_true = test_generator.classes

cm = confusion_matrix(y_true, y_pred)

print("Confusion Matrix:")
print(cm)

"""<br>
<h3>Compute the Classification Report</h3>
"""

cr = classification_report(y_true, y_pred, target_names=validation_generator.class_indices.keys())

print("Classification Report:")
print(cr)

"""<br>
<h3>Receiver Operating Characteristic (ROC) and the Area Under the Curve (AUC)</h3>
"""

fpr, tpr, _ = roc_curve(y_true, y_pred)

roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""<br>

<h2 align=center id="upload_pred">Upload and Predict Your Picture!</h2>

<p>You can upload any image and have the model predict whether it's a dog or a cat.</p>
<ul>
  <li>Find an image of a dog or cat.</li>
  <li>Run the following code cell.  It will ask you to upload an image.</li>
  <li>The model will print "is a dog" or "is a cat" depending on the model's prediction.</li>
<ul>
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
 
  path = '/content/' + fn
  img = load_img(path, target_size=image_resize)
  x = img_to_array(img)
  x = np.expand_dims(x, axis=0)

  image_tensor = np.vstack([x])
  classes = model_vgg16.predict(image_tensor)

  print(classes)
  print(classes[0])
  if classes[0] > 0.5:
    print(fn + " is a dog")
  else:
    print(fn + " is a cat")

from google.colab import files
files.download('/content/variables/variables.data-00000-of-00001')
files.download('/content/variables/variables.index')
files.download('/content/best_model.h5')
files.download('/content/fingerprint.pb')
files.download('/content/keras_metadata.pb')
files.download('/content/saved_model.pb')
files.download('/content/training.csv')

"""<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")